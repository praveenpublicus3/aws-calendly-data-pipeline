{
	"jobConfig": {
		"name": "calendly_spend_ingestion",
		"description": "",
		"role": "arn:aws:iam::647132523501:role/glue-s3-role-calendly",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "calendly_spend_ingestion.py",
		"scriptLocation": "s3://aws-glue-assets-647132523501-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-10-30T13:16:55.499Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-647132523501-us-east-1/temporary/",
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-647132523501-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys
from awsglue.utils import getResolvedOptions
from awsglue.context import GlueContext
from pyspark.context import SparkContext
from awsglue.job import Job
from pyspark.sql import functions as F

args = getResolvedOptions(sys.argv, ['JOB_NAME'])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args['JOB_NAME'], args)

# 1️⃣ Read raw spend data from S3
input_path = "s3://marketing-calendly-project/spend/"
df = spark.read.option("multiline", "true").json(input_path)

# 2️⃣ Flatten array<struct> if needed
if "array" in df.columns:
    flat_df = df.select(F.explode("array").alias("item")).select(
        F.col("item.date").alias("date"),
        F.col("item.channel").alias("channel"),
        F.col("item.spend").alias("spend")
    )
else:
    flat_df = df.select("date", "channel", "spend")

# 3️⃣ Clean data and cast types
flat_df = flat_df.withColumn("date", F.to_date("date")) \
                 .withColumn("channel", F.col("channel").cast("string")) \
                 .withColumn("spend", F.col("spend").cast("double")) \
                 .na.drop(subset=["channel", "spend"])

# 4️⃣ Write flattened data to Gold zone
output_path = "s3://marketing-calendly-project/gold/spend_flattened/"
flat_df.write.mode("overwrite").parquet(output_path)

print("✅ Spend data successfully flattened and written to Gold zone.")

job.commit()"
}
