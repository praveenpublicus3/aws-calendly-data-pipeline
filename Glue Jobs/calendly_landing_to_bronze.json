from awsglue.context import GlueContext
from awsglue.job import Job
from pyspark.context import SparkContext
from pyspark.sql import functions as F

sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)

# Step 1: Read from landing folder
landing_df = glueContext.create_dynamic_frame.from_options(
    connection_type="s3",
    connection_options={"paths": ["s3://marketing-calendly-project/landing/"]},
    format="json"
).toDF()

# Step 2: Flatten the simple structure
flattened_df = landing_df.select(
    F.col("event"),
    F.col("payload.email").alias("invitee_email"),
    F.col("payload.name").alias("invitee_name")
)

# Step 3: Write to Bronze zone
flattened_df.write.mode("overwrite").json("s3://marketing-calendly-project/bronze/calendly/")

job.commit()
